{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diegomrodrigues/my_gpt/blob/main/GPT-2%20Fine-Tunned%20for%20Sentence%20Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Como aproveitar ao máximo sua assinatura do Colab\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/github/diegomrodrigues/my_gpt/blob/main/GT2%20Starter%20Implementation.ipynb\n",
        "\"\"\"\n",
        "\n",
        "!pip install tiktoken datasets --quiet\n",
        "\n",
        "import torch\n",
        "import math\n",
        "from torch import nn\n",
        "\n",
        "def gelu(x):\n",
        "    return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, hidden_size, eps=1e-12):\n",
        "        super(LayerNorm, self).__init__()\n",
        "\n",
        "        # Parâmetros aprendíveis para escala (gamma) e deslocamento (beta)\n",
        "        # Inicializados com 1s e 0s respectivamente\n",
        "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
        "        self.bias = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        # Epsilon para estabilidade numérica\n",
        "        self.epsilon = eps\n",
        "\n",
        "    def forward(self, x):  # x: [batch_size, seq_length, hidden_size]\n",
        "        # Calcula a média ao longo da última dimensão (feature dimension)\n",
        "        # keepdim=True mantém a dimensionalidade para broadcast correto\n",
        "        mu = x.mean(-1, keepdim=True)  # [batch_size, seq_length, 1]\n",
        "\n",
        "        # Calcula a variância\n",
        "        # Usa a fórmula E[(X - μ)^2] para variância\n",
        "        sigma = (x - mu).pow(2).mean(-1, keepdim=True)  # [batch_size, seq_length, 1]\n",
        "\n",
        "        # Normalização: (x - μ) / sqrt(σ^2 + ε)\n",
        "        # Epsilon (ε) evita divisão por zero\n",
        "        x = (x - mu) / torch.sqrt(sigma + self.epsilon)  # [batch_size, seq_length, hidden_size]\n",
        "\n",
        "        # Aplica transformação afim com parâmetros aprendíveis\n",
        "        # y = γ * x + β, onde γ = self.weight e β = self.bias\n",
        "        return self.weight * x + self.bias  # [batch_size, seq_length, hidden_size]\n",
        "\n",
        "class Conv1D(nn.Module):\n",
        "    def __init__(self, nf, nx):\n",
        "        # nf: número de filtros (saída)\n",
        "        # nx: tamanho da entrada\n",
        "        super(Conv1D, self).__init__()\n",
        "\n",
        "        self.nf = nf\n",
        "\n",
        "        # Inicializa os pesos com uma distribuição normal\n",
        "        # [nx, nf]\n",
        "        w = torch.empty(nx, nf)\n",
        "        nn.init.normal_(w, std=0.02)\n",
        "\n",
        "        # Cria parâmetros treináveis para pesos e vieses\n",
        "        self.weight = nn.Parameter(w)  # [nx, nf]\n",
        "        self.bias = nn.Parameter(torch.zeros(nf))  # [nf]\n",
        "\n",
        "    def forward(self, x): # x: [batch_size, input_len, nx]\n",
        "\n",
        "        # Prepara o shape de saída\n",
        "        size_out = x.size()[:-1] + (self.nf,) # [batch_size, input_len, nf]\n",
        "\n",
        "        # Reshape x para 2D\n",
        "        x_2d = x.view(-1, x.size(-1)) # [batch_size * input_len, nx]\n",
        "\n",
        "        # Aplica a transformação linear\n",
        "        # torch.addmm realiza: out = beta * self.bias + alpha * (x_2d @ self.weight)\n",
        "        x_transformed = torch.addmm(self.bias, x_2d, self.weight) # [batch_size * input_len, nf]\n",
        "\n",
        "        # Reshape de volta para 3D\n",
        "        x_output = x_transformed.view(*size_out) # [batch_size, input_len, nf]\n",
        "\n",
        "        return x_output\n",
        "\n",
        "class SingleHeadAttention(nn.Module):\n",
        "    def __init__(self, nx, n_ctx, scale=False):\n",
        "        super(SingleHeadAttention, self).__init__()\n",
        "\n",
        "        # nx: dimensão do modelo (tamanho dos embeddings)\n",
        "        self.nx = nx\n",
        "        # n_ctx: comprimento máximo do contexto (número máximo de tokens na sequência)\n",
        "        self.n_ctx = n_ctx\n",
        "\n",
        "        # Performar scaled dot product?\n",
        "        self.scale = scale\n",
        "\n",
        "        # Criamos uma máscara de atenção triangular inferior (causal)\n",
        "        # Isso garante que cada token só preste atenção aos tokens anteriores\n",
        "        # Shape: [1, 1, n_ctx, n_ctx]\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx))\n",
        "\n",
        "        # Camada linear para projetar a entrada em Query, Key, Value\n",
        "        # Multiplica por 3 porque criamos Q, K, V de uma vez\n",
        "        self.c_attn = Conv1D(nx * 3, nx)\n",
        "        # Camada linear para projetar a saída da atenção de volta ao espaço do modelo\n",
        "        self.c_proj = Conv1D(nx, nx)\n",
        "\n",
        "    def _attention(self, q, k, v):\n",
        "        # Calculamos os scores de atenção: Query * Key^T\n",
        "        # Isso mede quanto cada token (Query) deve prestar atenção a cada outro token (Key)\n",
        "        w = torch.matmul(q, k)  # [batch_size, 1, n_ctx, n_ctx]\n",
        "\n",
        "        # Aplicamos escala opcional para estabilizar o treinamento\n",
        "        # Dividimos pelo sqrt da dimensão para evitar que os gradientes fiquem muito grandes\n",
        "        if self.scale:\n",
        "            w = w / math.sqrt(v.size(-1))\n",
        "\n",
        "        # Preparamos a máscara causal\n",
        "        # Isso garante que não olhamos para tokens futuros\n",
        "        nd, ns = w.size(-2), w.size(-1)\n",
        "        mask = self.bias[:,:,ns-nd:ns,:ns]\n",
        "\n",
        "        # Aplicamos a máscara causal\n",
        "        # Colocamos -infinito onde a máscara é 0, efetivamente zerando esses scores no softmax\n",
        "        w = w * mask - 1e10 * (1 - mask)\n",
        "\n",
        "        # Aplicamos softmax para obter os pesos de atenção\n",
        "        # Isso normaliza os scores para que somem 1 para cada query\n",
        "        w = nn.Softmax(dim=-1)(w)\n",
        "\n",
        "        # Calculamos a saída da atenção: pesos de atenção * Values\n",
        "        # Isso agrega as informações dos tokens relevantes\n",
        "        output = torch.matmul(w, v)  # [batch_size, 1, n_ctx, nx]\n",
        "\n",
        "        return output\n",
        "\n",
        "    def forward(self, x, layer_past=None):\n",
        "        # x: entrada [batch_size, n_ctx, nx]\n",
        "\n",
        "        # Projetamos a entrada para Query, Key, Value de uma vez\n",
        "        qkv = self.c_attn(x)  # [batch_size, n_ctx, nx*3]\n",
        "\n",
        "        # Separamos Q, K, V\n",
        "        query, key, value = qkv.split(self.nx, dim=2)  # cada um: [batch_size, n_ctx, nx]\n",
        "\n",
        "        # Reshape para adicionar dimensão de cabeça (neste caso, apenas 1)\n",
        "        # Isso prepara os tensores para a operação de atenção\n",
        "        query = query.unsqueeze(1)  # [batch_size, 1, n_ctx, nx]\n",
        "        key = key.unsqueeze(1).transpose(-1, -2)  # [batch_size, 1, nx, n_ctx]\n",
        "        value = value.unsqueeze(1)  # [batch_size, 1, n_ctx, nx]\n",
        "\n",
        "        # Lidamos com o cache do estado passado, se fornecido\n",
        "        # Isso é útil para geração incremental de texto\n",
        "        if layer_past:\n",
        "            past_key, past_value = layer_past\n",
        "            key = torch.cat((past_key, key), dim=-1)\n",
        "            value = torch.cat((past_value, value), dim=-2)\n",
        "\n",
        "        # Armazenamos o estado atual para uso futuro\n",
        "        present = torch.stack((key.transpose(-1, -2), value))\n",
        "\n",
        "        # Calculamos a atenção\n",
        "        attn_output = self._attention(query, key, value)  # [batch_size, 1, n_ctx, nx]\n",
        "\n",
        "        # Removemos a dimensão da cabeça (que era 1)\n",
        "        attn_output = attn_output.squeeze(1)  # [batch_size, n_ctx, nx]\n",
        "        # Aplicamos a projeção final para voltar ao espaço do modelo\n",
        "        attn_output = self.c_proj(attn_output)  # [batch_size, n_ctx, nx]\n",
        "\n",
        "        return attn_output, present\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, nx, n_ctx, n_head, scale=False):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        # Cria uma máscara de atenção triangular inferior (causal)\n",
        "        # Isso garante que cada token só preste atenção aos tokens anteriores\n",
        "        # Shape: [1, 1, n_ctx, n_ctx]\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx))\n",
        "\n",
        "        self.n_head = n_head  # Número de cabeças de atenção\n",
        "        self.split_size = nx  # Tamanho da dimensão do modelo\n",
        "        self.scale = scale    # Flag para aplicar escala nos scores de atenção\n",
        "\n",
        "        # Camada linear para projetar a entrada em Query, Key, Value para todas as cabeças\n",
        "        self.c_attn = Conv1D(nx * 3, nx)\n",
        "        # Camada linear para projetar a saída da atenção de volta ao espaço do modelo\n",
        "        self.c_proj = Conv1D(nx, nx)\n",
        "\n",
        "    def _attention(self, q, k, v):\n",
        "        # Calcula os scores de atenção: Query * Key^T\n",
        "        # Shape: [batch_size, n_head, seq_len, seq_len]\n",
        "        w = torch.matmul(q, k)\n",
        "\n",
        "        # Aplica escala opcional para estabilizar o treinamento\n",
        "        if self.scale:\n",
        "            w = w / math.sqrt(v.size(-1))\n",
        "\n",
        "        # Prepara e aplica a máscara causal\n",
        "        nd, ns = w.size(-2), w.size(-1)\n",
        "        mask = self.bias[:, :, ns-nd:ns, :ns]\n",
        "        w = w * mask - 1e10 * (1 - mask)  # Aplica -inf onde a máscara é 0\n",
        "\n",
        "        # Aplica softmax para obter os pesos de atenção\n",
        "        w = nn.Softmax(dim=-1)(w)\n",
        "\n",
        "        # Calcula a saída da atenção: pesos de atenção * Values\n",
        "        output = torch.matmul(w, v)\n",
        "        return output\n",
        "\n",
        "    def _merge_heads(self, x):\n",
        "        # Reorganiza o tensor de [batch, head, seq, features] para [batch, seq, head*features]\n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        new_x_shape = x.size()[:-2] + (x.size(-2) * x.size(-1),)\n",
        "        return x.view(*new_x_shape)\n",
        "\n",
        "    def _split_heads(self, x, k=False):\n",
        "        # Divide o último dimensão em [n_head, features/n_head]\n",
        "        new_x_shape = x.size()[:-1] + (self.n_head, x.size(-1) // self.n_head)\n",
        "        x = x.view(*new_x_shape)\n",
        "\n",
        "        # Reorganiza o tensor para [batch, head, seq, features/n_head]\n",
        "        if k:\n",
        "            # Para as keys, colocamos a dim seq por último para otimizar a multiplicação de matrizes\n",
        "            return x.permute(0, 2, 3, 1)\n",
        "        else:\n",
        "            return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, x, layer_past=None):\n",
        "        # x: entrada [batch_size, seq_len, nx]\n",
        "\n",
        "        # Projeta a entrada para Q, K, V de uma vez\n",
        "        qkv = self.c_attn(x)  # [batch_size, seq_len, nx*3]\n",
        "\n",
        "        # Separa Q, K, V\n",
        "        query, key, value = qkv.split(self.split_size, dim=2)\n",
        "        # query, key, value: cada um [batch_size, seq_len, nx]\n",
        "\n",
        "        # Divide as cabeças e reorganiza\n",
        "        query = self._split_heads(query)  # [batch_size, n_head, seq_len, nx/n_head]\n",
        "        key = self._split_heads(key, k=True)  # [batch_size, n_head, nx/n_head, seq_len]\n",
        "        value = self._split_heads(value)  # [batch_size, n_head, seq_len, nx/n_head]\n",
        "\n",
        "        # Lida com o cache do estado passado, se fornecido\n",
        "        if layer_past:\n",
        "            past_key, past_value = layer_past[0].transpose(-2, -1), layer_past[1]\n",
        "            key = torch.cat((past_key, key), dim=-1)  # [batch_size, n_head, nx/n_head, seq_len_extended]\n",
        "            value = torch.cat((past_value, value), dim=-2)  # [batch_size, n_head, seq_len_extended, nx/n_head]\n",
        "\n",
        "        # Armazena o estado atual para uso futuro\n",
        "        present = torch.stack((key.transpose(-2, -1), value))\n",
        "        # present: [2, batch_size, n_head, seq_len, nx/n_head]\n",
        "\n",
        "        # Calcula a atenção para todas as cabeças\n",
        "        attn_output = self._attention(query, key, value)\n",
        "        # [batch_size, n_head, seq_len, nx/n_head]\n",
        "\n",
        "        # Combina as cabeças novamente\n",
        "        attn_output = self._merge_heads(attn_output)  # [batch_size, seq_len, nx]\n",
        "\n",
        "        # Projeta de volta para o espaço do modelo\n",
        "        attn_output = self.c_proj(attn_output)  # [batch_size, seq_len, nx]\n",
        "\n",
        "        return attn_output, present\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, n_state, n_embed):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        # n_state: geralmente 4 * n_embed, seguindo a arquitetura original do Transformer\n",
        "        # n_embed: dimensão do modelo (embedding dimension)\n",
        "\n",
        "        # Primeira camada linear: expande a dimensão\n",
        "        self.c_fc = Conv1D(n_state, n_embed)\n",
        "\n",
        "        # Segunda camada linear: projeta de volta para a dimensão original\n",
        "        self.c_proj = Conv1D(n_embed, n_state)\n",
        "\n",
        "        # Função de ativação GELU (Gaussian Error Linear Unit)\n",
        "        self.activation = gelu\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: entrada [batch_size, seq_len, n_embed]\n",
        "\n",
        "        # Aplica a primeira transformação linear e a função de ativação\n",
        "        h = self.c_fc(x)  # [batch_size, seq_len, n_state]\n",
        "        h = self.activation(h)  # [batch_size, seq_len, n_state]\n",
        "\n",
        "        # Aplica a segunda transformação linear\n",
        "        h2 = self.c_proj(h)  # [batch_size, seq_len, n_embed]\n",
        "\n",
        "        return h2  # [batch_size, seq_len, n_embed]\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, n_ctx, config, scale=False):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "\n",
        "        nx     = config.n_embed  # Dimensão do modelo\n",
        "        n_head = config.n_head   # Número de heads\n",
        "\n",
        "        # Primeira camada de normalização, aplicada antes da atenção\n",
        "        self.ln_1 = LayerNorm(nx, eps=config.layer_norm_epsilon)\n",
        "\n",
        "        # Camada de atenção (neste caso, atenção de cabeça única)\n",
        "        if n_head <= 1:\n",
        "            self.attention = SingleHeadAttention(nx, n_ctx, scale)\n",
        "        else:\n",
        "            self.attention = MultiHeadAttention(nx, n_ctx, n_head, scale)\n",
        "\n",
        "        # Segunda camada de normalização, aplicada antes da MLP\n",
        "        self.ln_2 = LayerNorm(nx, eps=config.layer_norm_epsilon)\n",
        "\n",
        "        # MLP (Feed-Forward Network)\n",
        "        self.mlp = MLP(4 * nx, nx)\n",
        "\n",
        "    def forward(self, x, layer_past=None):\n",
        "        # x: entrada [batch_size, seq_len, nx]\n",
        "\n",
        "        # Primeira normalização de camada\n",
        "        normalized_x = self.ln_1(x)  # [batch_size, seq_len, nx]\n",
        "\n",
        "        # Camada de atenção\n",
        "        attention_output, present = self.attention(normalized_x, layer_past=layer_past)\n",
        "        # attention_output: [batch_size, seq_len, nx]\n",
        "        # present: estado cacheado para geração incremental\n",
        "\n",
        "        # Conexão residual após a atenção\n",
        "        x = x + attention_output  # [batch_size, seq_len, nx]\n",
        "\n",
        "        # Segunda normalização de camada\n",
        "        normalized_x = self.ln_2(x)  # [batch_size, seq_len, nx]\n",
        "\n",
        "        # MLP\n",
        "        mlp_output = self.mlp(normalized_x)  # [batch_size, seq_len, nx]\n",
        "\n",
        "        # Conexão residual após a MLP\n",
        "        x = x + mlp_output  # [batch_size, seq_len, nx]\n",
        "\n",
        "        return x, present\n",
        "\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_layer = config.n_layer  # Número de camadas do Transformer\n",
        "        self.n_embed = config.n_embed  # Dimensão do embedding\n",
        "        self.n_vocab = config.n_vocab  # Tamanho do vocabulário\n",
        "        self.n_pos   = config.n_pos    # Número máximo de posições\n",
        "\n",
        "        # Embedding de tokens\n",
        "        self.wte = nn.Embedding(self.n_vocab, self.n_embed)\n",
        "        # Embedding de posições\n",
        "        self.wpe = nn.Embedding(self.n_pos, self.n_embed)\n",
        "\n",
        "        # Cria um bloco do Transformer\n",
        "        block = TransformerBlock(config.n_ctx, config, scale=True)\n",
        "\n",
        "        # Cria uma lista de blocos do Transformer\n",
        "        self.h = nn.ModuleList([copy.deepcopy(block) for _ in range(self.n_layer)])\n",
        "\n",
        "        # Camada final de normalização\n",
        "        self.ln_f = LayerNorm(self.n_embed, eps=config.layer_norm_epsilon)\n",
        "\n",
        "    def forward(self, input_ids, position_ids=None, token_type_ids=None, past=None):\n",
        "        # input_ids: [batch_size, seq_len]\n",
        "\n",
        "        if past is None:\n",
        "            past_length = 0\n",
        "            past = [None] * len(self.h)\n",
        "        else:\n",
        "            past_length = past[0][0].size(-2)\n",
        "\n",
        "        if position_ids is None:\n",
        "            # Gera ids de posição se não fornecidos\n",
        "            position_ids = torch.arange(past_length, input_ids.size(-1) + past_length, dtype=torch.long, device=input_ids.device)\n",
        "            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)  # [batch_size, seq_len]\n",
        "\n",
        "        input_shape = input_ids.size()\n",
        "        input_ids = input_ids.view(-1, input_ids.size(-1))  # [batch_size * seq_len]\n",
        "        position_ids = position_ids.view(-1, position_ids.size(-1))  # [batch_size * seq_len]\n",
        "\n",
        "        # Aplica embeddings de tokens e posições\n",
        "        input_embeds = self.wte(input_ids)  # [batch_size * seq_len, n_embed]\n",
        "        position_embeds = self.wpe(position_ids)  # [batch_size * seq_len, n_embed]\n",
        "\n",
        "        if token_type_ids is not None:\n",
        "            token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1))\n",
        "            token_type_embeds = self.wte(token_type_ids)  # [batch_size * seq_len, n_embed]\n",
        "        else:\n",
        "            token_type_embeds = 0\n",
        "\n",
        "        # Soma todos os embeddings\n",
        "        hidden_states = input_embeds + position_embeds + token_type_embeds  # [batch_size * seq_len, n_embed]\n",
        "\n",
        "        presents = []\n",
        "        for block, layer_past in zip(self.h, past):\n",
        "            hidden_states, present = block(hidden_states, layer_past)\n",
        "            # hidden_states: [batch_size * seq_len, n_embed]\n",
        "            presents.append(present)\n",
        "\n",
        "        # Aplica a normalização final\n",
        "        hidden_states = self.ln_f(hidden_states)  # [batch_size * seq_len, n_embed]\n",
        "\n",
        "        # Reshape para a forma original\n",
        "        output_shape = input_shape + (hidden_states.size(-1),)\n",
        "        hidden_states = hidden_states.view(*output_shape)  # [batch_size, seq_len, n_embed]\n",
        "\n",
        "        return hidden_states, presents\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class LinearReadoutHead(nn.Module):\n",
        "    \"\"\"\n",
        "    Cabeça de leitura linear para o modelo GPT.\n",
        "    Esta classe é responsável por projetar os estados ocultos de volta para o espaço do vocabulário.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_embedding_weights: nn.Parameter, config: 'GPTConfig'):\n",
        "        super().__init__()\n",
        "        # Dimensão dos embeddings\n",
        "        self.n_embed = config.n_embed\n",
        "        # Tamanho do vocabulário\n",
        "        self.n_vocab = config.n_vocab\n",
        "        self.set_embedding_weights(model_embedding_weights)\n",
        "\n",
        "    def set_embedding_weights(self, model_embedding_weights: nn.Parameter):\n",
        "        # Cria a camada linear do decodificador sem viés\n",
        "        self.decoder = nn.Linear(self.n_embed, self.n_vocab, bias=False)\n",
        "\n",
        "        # Define os pesos do decodificador como os pesos de embedding\n",
        "        self.decoder.weight = model_embedding_weights\n",
        "\n",
        "    def forward(self, hidden_state: torch.Tensor) -> torch.Tensor:\n",
        "        lm_logits = self.decoder(hidden_state)\n",
        "        return lm_logits\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    # Parâmetros do modelo\n",
        "    n_vocab: int = 50257  # Tamanho do vocabulário (número de tokens únicos)\n",
        "    n_pos: int = 1024     # Tamanho máximo da sequência para embeddings posicionais\n",
        "    n_ctx: int = 1024     # Tamanho do contexto (comprimento máximo da sequência de entrada)\n",
        "    n_embed: int = 768    # Dimensão dos embeddings e camadas ocultas\n",
        "    n_layer: int = 12     # Número de camadas do transformer (blocos de atenção + feed-forward)\n",
        "    n_head: int = 12      # Número de cabeças de atenção em cada camada\n",
        "    n_inner: int = field(init=False)  # Dimensão da camada feed-forward, calculada no __post_init__\n",
        "\n",
        "    # Hiperparâmetros de treinamento\n",
        "    layer_norm_epsilon: float = 1e-5      # Epsilon para estabilidade numérica na normalização de camada\n",
        "    initializer_range: float = 0.02       # Intervalo para inicialização aleatória dos pesos\n",
        "    use_scaled_attention: bool = True     # Se True, usa atenção de produto escalar escalado\n",
        "\n",
        "    # Configurações de treinamento\n",
        "    batch_size: int = 32              # Número de amostras processadas em cada lote\n",
        "    learning_rate: float = 1e-4       # Taxa de aprendizagem para o otimizador\n",
        "    num_epochs: int = 10              # Número total de épocas de treinamento\n",
        "    num_labels: int = 3               # Número de classes para classificação (se aplicável)\n",
        "\n",
        "    # Regularização\n",
        "    resid_pdrop: float = 0.1  # Taxa de dropout para saídas residuais\n",
        "    embd_pdrop: float = 0.1   # Taxa de dropout para embeddings\n",
        "    attn_pdrop: float = 0.1   # Taxa de dropout para atenção\n",
        "\n",
        "    # Tokens especiais\n",
        "    pad_token_id: int = 50256  # ID do token de padding\n",
        "    bos_token_id: int = 50256  # ID do token de início de sequência (Beginning of Sequence)\n",
        "    eos_token_id: int = 50256  # ID do token de fim de sequência (End of Sequence)\n",
        "\n",
        "    # Otimização e checkpointing\n",
        "    accumulation_steps: int = 4       # Número de passos para acumulação de gradiente\n",
        "    patience: int = 3                 # Número de épocas para esperar antes de early stopping\n",
        "    checkpoint_dir: str = \"checkpoints\"  # Diretório para salvar checkpoints do modelo\n",
        "    log_dir: str = \"logs\"             # Diretório para salvar logs de treinamento\n",
        "\n",
        "    # Identificação do modelo\n",
        "    hub_model_id: str = None # ID do modelo no Hugging Face Hub\n",
        "\n",
        "    def __post_init__(self):\n",
        "        # Calcula a dimensão interna da camada feed-forward como 4 * n_embed\n",
        "        # Esta é uma prática comum em arquiteturas GPT\n",
        "        self.n_inner = 4 * self.n_embed\n",
        "\n",
        "class GPT2(nn.Module):\n",
        "\n",
        "    def __init__(self, config: 'GPTConfig'):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.transformer = Transformer(config)\n",
        "        self.readout_head = LinearReadoutHead(self.transformer.wte.weight, config)\n",
        "\n",
        "    def set_tied(self):\n",
        "        \"\"\"\n",
        "        Vincula os pesos da camada de embedding com a cabeça de leitura linear.\n",
        "        Isso implementa o compartilhamento de pesos entre a entrada e a saída do modelo.\n",
        "        \"\"\"\n",
        "        self.readout_head.set_embedding_weights(self.transformer.wte.weight)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.LongTensor,\n",
        "        position_ids: torch.LongTensor = None,\n",
        "        token_type_ids: torch.LongTensor = None,\n",
        "        past: torch.Tensor = None\n",
        "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Realiza a passagem para frente do modelo GPT-2.\n",
        "\n",
        "        :param input_ids: IDs dos tokens de entrada\n",
        "        :param position_ids: IDs das posições (opcional)\n",
        "        :param token_type_ids: IDs dos tipos de token (opcional)\n",
        "        :param past: Estado passado para geração incremental (opcional)\n",
        "        :return: Uma tupla contendo os logits de saída e os estados presentes\n",
        "        \"\"\"\n",
        "        # Passa a entrada pelo transformer\n",
        "        hidden_states, presents = self.transformer(input_ids, position_ids, token_type_ids, past)\n",
        "\n",
        "        # Passa os estados ocultos pela cabeça de leitura linear\n",
        "        logits = self.readout_head(hidden_states)\n",
        "\n",
        "        return logits, presents\n",
        "\n",
        "from typing import Optional\n",
        "\n",
        "class GPT2ForSequenceClassification(nn.Module):\n",
        "    def __init__(self, config: 'GPTConfig'):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.num_labels = config.num_labels\n",
        "        self.transformer = Transformer(config)\n",
        "        self.dropout = nn.Dropout(config.resid_pdrop)\n",
        "        self.classifier = nn.Linear(config.n_embed, self.num_labels)\n",
        "        self._init_weights(self.classifier)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.LongTensor] = None,\n",
        "        attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        token_type_ids: Optional[torch.LongTensor] = None,\n",
        "        position_ids: Optional[torch.LongTensor] = None,\n",
        "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        labels: Optional[torch.LongTensor] = None,\n",
        "    ):\n",
        "        transformer_outputs = self.transformer(\n",
        "            input_ids,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids\n",
        "        )\n",
        "\n",
        "        hidden_states = transformer_outputs[0]  # [batch_size, sequence_length, hidden_size]\n",
        "\n",
        "        # Determine as dimensões do batch e da sequência\n",
        "        if input_ids is not None:\n",
        "            batch_size, sequence_length = input_ids.shape[:2]\n",
        "        else:\n",
        "            batch_size, sequence_length = inputs_embeds.shape[:2]\n",
        "\n",
        "        # Determine os comprimentos reais das sequências\n",
        "        if attention_mask is not None:\n",
        "            # Use a máscara de atenção para determinar o último token não-mascarado\n",
        "            sequence_lengths = attention_mask.sum(dim=1) - 1\n",
        "        elif self.config.pad_token_id is not None and input_ids is not None:\n",
        "            # Se não temos máscara de atenção, mas temos um token de padding, use-o\n",
        "            sequence_lengths = torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1\n",
        "        else:\n",
        "            # Se não temos nem máscara de atenção nem token de padding, assume que todas as sequências têm o mesmo comprimento\n",
        "            sequence_lengths = torch.tensor([sequence_length - 1] * batch_size, device=hidden_states.device)\n",
        "\n",
        "        # Garanta que sequence_lengths está no intervalo correto\n",
        "        sequence_lengths = torch.clamp(sequence_lengths, min=0, max=sequence_length - 1)\n",
        "\n",
        "        # Selecione o último estado oculto não-padding para cada sequência\n",
        "        pooled_output = hidden_states[torch.arange(batch_size, device=hidden_states.device), sequence_lengths]\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            if self.num_labels == 1:\n",
        "                # Regressão\n",
        "                loss_fct = nn.MSELoss()\n",
        "                loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
        "            else:\n",
        "                # Classificação\n",
        "                loss_fct = nn.CrossEntropyLoss()\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "        output = (logits,) + transformer_outputs[1:]\n",
        "        return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "    def freeze_transformer(self, n_blocks=None):\n",
        "        for param in self.transformer.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        if n_blocks:\n",
        "            for i in range(1, n_blocks + 1):\n",
        "                for param in self.transformer.h[-i].parameters():\n",
        "                    param.requires_grad = True\n",
        "\n",
        "    def unfreeze_transformer(self):\n",
        "        for param in self.transformer.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from huggingface_hub import HfApi, HfFolder\n",
        "from huggingface_hub import HFSummaryWriter\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import numpy as np\n",
        "import os\n",
        "from huggingface_hub import HfApi\n",
        "from huggingface_hub import HfFolder\n",
        "from huggingface_hub import HFSummaryWriter\n",
        "\n",
        "def train_gpt(model, train_loader, val_loader, config, device):\n",
        "    print(f\"Setting device to {device}\")\n",
        "    model.to(device)\n",
        "\n",
        "    num_training_steps = len(train_loader) * config.num_epochs\n",
        "    num_warmup_steps = int(0.1 * num_training_steps)\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=num_warmup_steps,\n",
        "        num_training_steps=num_training_steps\n",
        "    )\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    accumulation_steps = config.accumulation_steps\n",
        "    patience_counter = 0\n",
        "\n",
        "    # Set up Hugging Face Hub API\n",
        "    api = HfApi()\n",
        "    token = HfFolder.get_token()\n",
        "    if token is None:\n",
        "        raise ValueError(\"No Hugging Face token found. Please login using `huggingface-cli login`\")\n",
        "\n",
        "    # Set up HFSummaryWriter\n",
        "    writer = HFSummaryWriter(logdir=f\"runs/{config.checkpoint_dir}\", repo_id=config.hub_model_id, commit_every=5)\n",
        "\n",
        "    global_step = 0\n",
        "    for epoch in range(config.num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{config.num_epochs}\")\n",
        "\n",
        "        for step, batch in progress_bar:\n",
        "            input_ids, label_ids = [b.to(device) for b in batch]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(input_ids, labels=label_ids)\n",
        "            loss, logits = outputs[:2]\n",
        "            loss = loss / accumulation_steps\n",
        "            loss.backward()\n",
        "\n",
        "            if (step + 1) % accumulation_steps == 0:\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            total_loss += loss.item() * accumulation_steps\n",
        "\n",
        "            # Collect predictions and labels for metrics\n",
        "            preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "            labels = label_ids.cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels)\n",
        "\n",
        "            global_step = epoch * len(train_loader) + step\n",
        "\n",
        "            if global_step % 50 == 0:\n",
        "                # Log metrics every batch\n",
        "                writer.add_scalar(\"Loss/train\", loss.item(), global_step)\n",
        "                writer.add_scalar(\"Learning_rate\", scheduler.get_last_lr()[0], global_step)\n",
        "\n",
        "                # Calculate and log batch-level metrics\n",
        "                batch_accuracy = accuracy_score(labels, preds)\n",
        "                batch_f1 = f1_score(labels, preds, average='weighted')\n",
        "                writer.add_scalar(\"Accuracy/train\", batch_accuracy, global_step)\n",
        "                writer.add_scalar(\"F1_Score/train\", batch_f1, global_step)\n",
        "\n",
        "            # Update progress bar\n",
        "            progress_bar.set_postfix({\n",
        "                'loss': f\"{total_loss/(step+1):.4f}\",\n",
        "                'acc': f\"{batch_accuracy:.4f}\",\n",
        "                'f1': f\"{batch_f1:.4f}\"\n",
        "            })\n",
        "\n",
        "            # Intermediate checkpoint and upload to Hugging Face Hub\n",
        "            if (step + 1) % 1000 == 0:\n",
        "                checkpoint_path = f\"{config.checkpoint_dir}/checkpoint_step{global_step}.pt\"\n",
        "                torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'global_step': global_step,\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'loss': total_loss,\n",
        "                }, checkpoint_path)\n",
        "\n",
        "                # Upload checkpoint to Hugging Face Hub\n",
        "                api.upload_file(\n",
        "                    path_or_fileobj=checkpoint_path,\n",
        "                    path_in_repo=f\"{config.checkpoint_dir}/checkpoint_step{global_step}.pt\",\n",
        "                    repo_id=config.hub_model_id,\n",
        "                    token=token\n",
        "                )\n",
        "                print(f\"Uploaded checkpoint to {config.hub_model_id}\")\n",
        "\n",
        "                # Evaluation\n",
        "                val_loss, val_accuracy, val_f1 = evaluate(model, val_loader, device)\n",
        "\n",
        "                # Log validation metrics\n",
        "                writer.add_scalar(\"Loss/validation\", val_loss, global_step)\n",
        "                writer.add_scalar(\"Accuracy/validation\", val_accuracy, global_step)\n",
        "                writer.add_scalar(\"F1_Score/validation\", val_f1, global_step)\n",
        "\n",
        "                print(f\"Val Loss: {val_loss:.4f}, \"\n",
        "                      f\"Val Acc: {val_accuracy:.4f}, \"\n",
        "                      f\"Val F1: {val_f1:.4f}\")\n",
        "\n",
        "        # Calculate epoch-level metrics\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "        train_accuracy = accuracy_score(all_labels, all_preds)\n",
        "        train_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{config.num_epochs}, \"\n",
        "              f\"Train Loss: {avg_train_loss:.4f}, \"\n",
        "              f\"Train Acc: {train_accuracy:.4f}, \"\n",
        "              f\"Train F1: {train_f1:.4f}\")\n",
        "\n",
        "        # Evaluation\n",
        "        val_loss, val_accuracy, val_f1 = evaluate(model, val_loader, device)\n",
        "\n",
        "        # Log validation metrics\n",
        "        writer.add_scalar(\"Loss/validation\", val_loss, global_step)\n",
        "        writer.add_scalar(\"Accuracy/validation\", val_accuracy, global_step)\n",
        "        writer.add_scalar(\"F1_Score/validation\", val_f1, global_step)\n",
        "\n",
        "        print(f\"Val Loss: {val_loss:.4f}, \"\n",
        "              f\"Val Acc: {val_accuracy:.4f}, \"\n",
        "              f\"Val F1: {val_f1:.4f}\")\n",
        "\n",
        "        # Save the best model and upload to Hugging Face Hub\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_path = f\"{config.checkpoint_dir}/best_model.pt\"\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "\n",
        "            api.upload_file(\n",
        "                path_or_fileobj=best_model_path,\n",
        "                path_in_repo=f\"{config.checkpoint_dir}_best_model.pt\",\n",
        "                repo_id=config.hub_model_id,\n",
        "                token=token\n",
        "            )\n",
        "            print(f\"Uploaded best model to {config.hub_model_id}\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= config.patience:\n",
        "                print(\"Early stopping triggered\")\n",
        "                break\n",
        "\n",
        "    print(\"Training completed!\")\n",
        "    writer.close()\n",
        "\n",
        "def evaluate(model, val_loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids, label_ids = [b.to(device) for b in batch]\n",
        "\n",
        "            outputs = model(input_ids, labels=label_ids)\n",
        "            loss, logits = outputs[:2]\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "            labels = label_ids.cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels)\n",
        "\n",
        "    avg_loss = total_loss / len(val_loader)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "    return avg_loss, accuracy, f1\n",
        "\n",
        "def load_weight(model, state_dict):\n",
        "    old_keys = []\n",
        "    new_keys = []\n",
        "    for key in state_dict.keys():\n",
        "        new_key = None\n",
        "        if key.endswith(\".g\"):\n",
        "            new_key = key[:-2] + \".weight\"\n",
        "        elif key.endswith(\".b\"):\n",
        "            new_key = key[:-2] + \".bias\"\n",
        "        elif key.endswith(\".w\"):\n",
        "            new_key = key[:-2] + \".weight\"\n",
        "        if new_key:\n",
        "            old_keys.append(key)\n",
        "            new_keys.append(new_key)\n",
        "    for old_key, new_key in zip(old_keys, new_keys):\n",
        "        state_dict[new_key] = state_dict.pop(old_key)\n",
        "\n",
        "    # Mapeamento de nomes de camadas\n",
        "    layer_map = {\n",
        "        \"wte\": \"transformer.wte\",\n",
        "        \"wpe\": \"transformer.wpe\",\n",
        "        \"h\": \"transformer.h\",\n",
        "        \"ln_f\": \"transformer.ln_f\",\n",
        "    }\n",
        "\n",
        "    # Ajusta os nomes das chaves no state_dict\n",
        "    for old_key in list(state_dict.keys()):\n",
        "        parts = old_key.split('.')\n",
        "        if parts[0] in layer_map:\n",
        "            new_key = layer_map[parts[0]] + '.' + '.'.join(parts[1:])\n",
        "            state_dict[new_key] = state_dict.pop(old_key)\n",
        "\n",
        "    # Carrega os pesos no modelo\n",
        "    model.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "    return model\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from datasets import load_dataset\n",
        "\n",
        "class SetenceClassificationDataset(Dataset):\n",
        "    def __init__(self, split, tokenizer, max_length=None, pad_token_id=50256):\n",
        "        self.dynasent_r1 = load_dataset(\"dynabench/dynasent\", 'dynabench.dynasent.r1.all', trust_remote_code=True)\n",
        "        self.dynasent_r2 = load_dataset(\"dynabench/dynasent\", 'dynabench.dynasent.r2.all')\n",
        "        self.sst = load_dataset(\"SetFit/sst5\")\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        def convert_sst_label(s):\n",
        "            return s.split(\" \")[-1]\n",
        "\n",
        "        self.dataset = {}\n",
        "\n",
        "        self.label_ids = {\n",
        "            \"neutral\": 0,\n",
        "            \"positive\": 1,\n",
        "            \"negative\": 2\n",
        "        }\n",
        "\n",
        "        self.dataset[\"features\"] = []\n",
        "        self.dataset[\"label_ids\"] = []\n",
        "\n",
        "        self.dataset[\"features\"].extend([tokenizer.encode(s) for s in self.dynasent_r1[split][\"sentence\"]])\n",
        "        self.dataset[\"features\"].extend([tokenizer.encode(s) for s in self.dynasent_r2[split][\"sentence\"]])\n",
        "        self.dataset[\"features\"].extend([tokenizer.encode(s) for s in self.sst[split][\"text\"]])\n",
        "\n",
        "        self.dataset[\"label_ids\"].extend([self.label_ids[l] for l in self.dynasent_r1[split][\"gold_label\"]])\n",
        "        self.dataset[\"label_ids\"].extend([self.label_ids[l] for l in self.dynasent_r2[split][\"gold_label\"]])\n",
        "        self.dataset[\"label_ids\"].extend([self.label_ids[convert_sst_label(l)] for l in self.sst[split][\"label_text\"]])\n",
        "\n",
        "        self.max_length = max_length or self._longest_encoded_length()\n",
        "\n",
        "        self.dataset[\"padded_features\"] = [\n",
        "            features[:self.max_length] +\n",
        "            [pad_token_id] * (self.max_length - len(features))\n",
        "            for features in self.dataset[\"features\"]\n",
        "        ]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        encoded = self.dataset[\"padded_features\"][index]\n",
        "        label = self.dataset[\"label_ids\"][index]\n",
        "\n",
        "        return (\n",
        "            torch.tensor(encoded, dtype=torch.long),\n",
        "            torch.tensor(label, dtype=torch.long)\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset[\"padded_features\"])\n",
        "\n",
        "    def _longest_encoded_length(self):\n",
        "        return max(len(features) for features in self.dataset[\"features\"])\n",
        "\n",
        "import tiktoken\n",
        "\n",
        "tokenizer = tiktoken.encoding_for_model(\"gpt-2\")\n",
        "\n",
        "train_dataset = SetenceClassificationDataset(\"train\", tokenizer)\n",
        "val_dataset = SetenceClassificationDataset(\"validation\", tokenizer)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=48,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    dataset=val_dataset,\n",
        "    batch_size=48,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "import requests\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()\n",
        "\n",
        "def downloadGPT2Checkpoint():\n",
        "    if not os.path.exists('./gpt2-pytorch_model.bin'):\n",
        "        print(\"Downloading GPT-2 checkpoint...\")\n",
        "        url = 'https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin'\n",
        "        r = requests.get(url, allow_redirects=True)\n",
        "        open('gpt2-pytorch_model.bin', 'wb').write(r.content)\n",
        "    assert os.path.exists(\"./gpt2-pytorch_model.bin\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "# Exemplo de uso\n",
        "config = GPTConfig(\n",
        "    hub_model_id=\"diegomrodrigues/GPT2-Fine-Tuned-Classification\",\n",
        "    batch_size=48,\n",
        "    learning_rate=3e-5,\n",
        "    num_epochs=50,\n",
        "    accumulation_steps=2,\n",
        "    patience=10,\n",
        "    checkpoint_dir=\"checkpoints_5blocks_from_32904_7380\"\n",
        ")\n",
        "\n",
        "model = GPT2ForSequenceClassification(config)\n",
        "\n",
        "#state_dict = torch.load('/content/checkpoints_3blocks_from_32904_7380/checkpoint_step999.pt')\n",
        "#model.load_state_dict(state_dict['model_state_dict'])\n",
        "\n",
        "#model.freeze_transformer(n_blocks=5)\n",
        "\n",
        "#train_gpt(model, train_loader, val_loader, config, device)\n"
      ],
      "metadata": {
        "id": "l7k5Ti-n5I-h",
        "outputId": "8773679a-6d11-4797-e2fa-01d0e4767ecc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671,
          "referenced_widgets": [
            "61434b08e51f4f23bf985cd9a977c425",
            "87e8b5a2c0154d0ca80b42b43967fc47",
            "3fefeca8811e4bd996e38b5381781e28",
            "6e0a367f5c2945478ba5d72c02964f6a",
            "0fffbfdc9ed4425f8d21045d2d3845cc",
            "2a44288a5f1a4b18835790aa72bc3245",
            "4719bc639a6a456289e47eb8c63ec749",
            "d661eb2335ce446eb373f851dfcf846b",
            "52a121a8bb3442fe82572872faad8db4",
            "ca6ae8fdb69741fd9091124be83970b7",
            "c08c60c6d7f54c6dba8f2373bc290ad9",
            "3ff33b9737fc4f94a31d7049dc31e667",
            "f1cd90f31d7846c490b750294534854f",
            "5470c73e08614a1a87e7883b5da5c222",
            "344d383e7520432eb84d9e67ef71f1da",
            "2cc0812162964d73a1854b1800afb0cb",
            "00bcb575eedf4165b9148bf04206ae4c"
          ]
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n",
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n",
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n",
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n",
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n",
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "61434b08e51f4f23bf985cd9a977c425"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V8tMNWVDAXdJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Como aproveitar ao máximo sua assinatura do Colab",
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "61434b08e51f4f23bf985cd9a977c425": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_87e8b5a2c0154d0ca80b42b43967fc47",
              "IPY_MODEL_3fefeca8811e4bd996e38b5381781e28",
              "IPY_MODEL_6e0a367f5c2945478ba5d72c02964f6a",
              "IPY_MODEL_0fffbfdc9ed4425f8d21045d2d3845cc",
              "IPY_MODEL_2a44288a5f1a4b18835790aa72bc3245"
            ],
            "layout": "IPY_MODEL_4719bc639a6a456289e47eb8c63ec749"
          }
        },
        "87e8b5a2c0154d0ca80b42b43967fc47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d661eb2335ce446eb373f851dfcf846b",
            "placeholder": "​",
            "style": "IPY_MODEL_52a121a8bb3442fe82572872faad8db4",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "3fefeca8811e4bd996e38b5381781e28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_ca6ae8fdb69741fd9091124be83970b7",
            "placeholder": "​",
            "style": "IPY_MODEL_c08c60c6d7f54c6dba8f2373bc290ad9",
            "value": ""
          }
        },
        "6e0a367f5c2945478ba5d72c02964f6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_3ff33b9737fc4f94a31d7049dc31e667",
            "style": "IPY_MODEL_f1cd90f31d7846c490b750294534854f",
            "value": true
          }
        },
        "0fffbfdc9ed4425f8d21045d2d3845cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_5470c73e08614a1a87e7883b5da5c222",
            "style": "IPY_MODEL_344d383e7520432eb84d9e67ef71f1da",
            "tooltip": ""
          }
        },
        "2a44288a5f1a4b18835790aa72bc3245": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2cc0812162964d73a1854b1800afb0cb",
            "placeholder": "​",
            "style": "IPY_MODEL_00bcb575eedf4165b9148bf04206ae4c",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "4719bc639a6a456289e47eb8c63ec749": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "d661eb2335ce446eb373f851dfcf846b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52a121a8bb3442fe82572872faad8db4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca6ae8fdb69741fd9091124be83970b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c08c60c6d7f54c6dba8f2373bc290ad9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3ff33b9737fc4f94a31d7049dc31e667": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1cd90f31d7846c490b750294534854f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5470c73e08614a1a87e7883b5da5c222": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "344d383e7520432eb84d9e67ef71f1da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "2cc0812162964d73a1854b1800afb0cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00bcb575eedf4165b9148bf04206ae4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
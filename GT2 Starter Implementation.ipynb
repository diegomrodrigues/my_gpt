{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diegomrodrigues/my_gpt/blob/main/GT2%20Starter%20Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "from torch import nn\n",
        "\n",
        "def gelu(x):\n",
        "    return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))"
      ],
      "metadata": {
        "id": "zO3DhFuK3gQC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, hidden_size, eps=1e-12):\n",
        "        super(LayerNorm, self).__init__()\n",
        "\n",
        "        # Parâmetros aprendíveis para escala (gamma) e deslocamento (beta)\n",
        "        # Inicializados com 1s e 0s respectivamente\n",
        "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
        "        self.bias = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        # Epsilon para estabilidade numérica\n",
        "        self.epsilon = eps\n",
        "\n",
        "    def forward(self, x):  # x: [batch_size, seq_length, hidden_size]\n",
        "        # Calcula a média ao longo da última dimensão (feature dimension)\n",
        "        # keepdim=True mantém a dimensionalidade para broadcast correto\n",
        "        mu = x.mean(-1, keepdim=True)  # [batch_size, seq_length, 1]\n",
        "\n",
        "        # Calcula a variância\n",
        "        # Usa a fórmula E[(X - μ)^2] para variância\n",
        "        sigma = (x - mu).pow(2).mean(-1, keepdim=True)  # [batch_size, seq_length, 1]\n",
        "\n",
        "        # Normalização: (x - μ) / sqrt(σ^2 + ε)\n",
        "        # Epsilon (ε) evita divisão por zero\n",
        "        x = (x - mu) / torch.sqrt(sigma + self.epsilon)  # [batch_size, seq_length, hidden_size]\n",
        "\n",
        "        # Aplica transformação afim com parâmetros aprendíveis\n",
        "        # y = γ * x + β, onde γ = self.weight e β = self.bias\n",
        "        return self.weight * x + self.bias  # [batch_size, seq_length, hidden_size]\n"
      ],
      "metadata": {
        "id": "UQi_rebQ3yYr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv1D(nn.Module):\n",
        "    def __init__(self, nf, nx):\n",
        "        # nf: número de filtros (saída)\n",
        "        # nx: tamanho da entrada\n",
        "        super(Conv1D, self).__init__()\n",
        "\n",
        "        self.nf = nf\n",
        "\n",
        "        # Inicializa os pesos com uma distribuição normal\n",
        "        # [nx, nf]\n",
        "        w = torch.empty(nx, nf)\n",
        "        nn.init.normal_(w, std=0.02)\n",
        "\n",
        "        # Cria parâmetros treináveis para pesos e vieses\n",
        "        self.weight = nn.Parameter(w)  # [nx, nf]\n",
        "        self.bias = nn.Parameter(torch.zeros(nf))  # [nf]\n",
        "\n",
        "    def forward(self, x): # x: [batch_size, input_len, nx]\n",
        "\n",
        "        # Prepara o shape de saída\n",
        "        size_out = x.size()[:-1] + (self.nf,) # [batch_size, input_len, nf]\n",
        "\n",
        "        # Reshape x para 2D\n",
        "        x_2d = x.view(-1, x.size(-1)) # [batch_size * input_len, nx]\n",
        "\n",
        "        # Aplica a transformação linear\n",
        "        # torch.addmm realiza: out = beta * self.bias + alpha * (x_2d @ self.weight)\n",
        "        x_transformed = torch.addmm(self.bias, x_2d, self.weight) # [batch_size * input_len, nf]\n",
        "\n",
        "        # Reshape de volta para 3D\n",
        "        x_output = x_transformed.view(*size_out) # [batch_size, input_len, nf]\n",
        "\n",
        "        return x_output"
      ],
      "metadata": {
        "id": "SgKl1baE5tCX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SingleHeadAttention(nn.Module):\n",
        "    def __init__(self, nx, n_ctx, config, scale=False):\n",
        "        super(SingleHeadAttention, self).__init__()\n",
        "\n",
        "        # nx: dimensão do modelo (tamanho dos embeddings)\n",
        "        self.nx = nx\n",
        "        # n_ctx: comprimento máximo do contexto (número máximo de tokens na sequência)\n",
        "        self.n_ctx = n_ctx\n",
        "\n",
        "        # Performar scaled dot product?\n",
        "        self.scale = scale\n",
        "\n",
        "        # Criamos uma máscara de atenção triangular inferior (causal)\n",
        "        # Isso garante que cada token só preste atenção aos tokens anteriores\n",
        "        # Shape: [1, 1, n_ctx, n_ctx]\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx))\n",
        "\n",
        "        # Camada linear para projetar a entrada em Query, Key, Value\n",
        "        # Multiplica por 3 porque criamos Q, K, V de uma vez\n",
        "        self.c_attn = Conv1D(nx * 3, nx)\n",
        "        # Camada linear para projetar a saída da atenção de volta ao espaço do modelo\n",
        "        self.c_proj = Conv1D(nx, nx)\n",
        "\n",
        "    def _attention(self, q, k, v):\n",
        "        # Calculamos os scores de atenção: Query * Key^T\n",
        "        # Isso mede quanto cada token (Query) deve prestar atenção a cada outro token (Key)\n",
        "        w = torch.matmul(q, k)  # [batch_size, 1, n_ctx, n_ctx]\n",
        "\n",
        "        # Aplicamos escala opcional para estabilizar o treinamento\n",
        "        # Dividimos pelo sqrt da dimensão para evitar que os gradientes fiquem muito grandes\n",
        "        if self.scale:\n",
        "            w = w / math.sqrt(v.size(-1))\n",
        "\n",
        "        # Preparamos a máscara causal\n",
        "        # Isso garante que não olhamos para tokens futuros\n",
        "        nd, ns = w.size(-2), w.size(-1)\n",
        "        mask = self.bias[:,:,ns-nd:ns,:ns]\n",
        "\n",
        "        # Aplicamos a máscara causal\n",
        "        # Colocamos -infinito onde a máscara é 0, efetivamente zerando esses scores no softmax\n",
        "        w = w * mask - 1e10 * (1 - mask)\n",
        "\n",
        "        # Aplicamos softmax para obter os pesos de atenção\n",
        "        # Isso normaliza os scores para que somem 1 para cada query\n",
        "        w = nn.Softmax(dim=-1)(w)\n",
        "\n",
        "        # Calculamos a saída da atenção: pesos de atenção * Values\n",
        "        # Isso agrega as informações dos tokens relevantes\n",
        "        output = torch.matmul(w, v)  # [batch_size, 1, n_ctx, nx]\n",
        "\n",
        "        return output\n",
        "\n",
        "    def forward(self, x, layer_past=None):\n",
        "        # x: entrada [batch_size, n_ctx, nx]\n",
        "\n",
        "        # Projetamos a entrada para Query, Key, Value de uma vez\n",
        "        qkv = self.c_attn(x)  # [batch_size, n_ctx, nx*3]\n",
        "\n",
        "        # Separamos Q, K, V\n",
        "        query, key, value = qkv.split(self.nx, dim=2)  # cada um: [batch_size, n_ctx, nx]\n",
        "\n",
        "        # Reshape para adicionar dimensão de cabeça (neste caso, apenas 1)\n",
        "        # Isso prepara os tensores para a operação de atenção\n",
        "        query = query.unsqueeze(1)  # [batch_size, 1, n_ctx, nx]\n",
        "        key = key.unsqueeze(1).transpose(-1, -2)  # [batch_size, 1, nx, n_ctx]\n",
        "        value = value.unsqueeze(1)  # [batch_size, 1, n_ctx, nx]\n",
        "\n",
        "        # Lidamos com o cache do estado passado, se fornecido\n",
        "        # Isso é útil para geração incremental de texto\n",
        "        if layer_past:\n",
        "            past_key, past_value = layer_past\n",
        "            key = torch.cat((past_key, key), dim=-1)\n",
        "            value = torch.cat((past_value, value), dim=-2)\n",
        "\n",
        "        # Armazenamos o estado atual para uso futuro\n",
        "        present = torch.stack((key, value))\n",
        "\n",
        "        # Calculamos a atenção\n",
        "        attn_output = self._attention(query, key, value)  # [batch_size, 1, n_ctx, nx]\n",
        "\n",
        "        # Removemos a dimensão da cabeça (que era 1)\n",
        "        attn_output = attn_output.squeeze(1)  # [batch_size, n_ctx, nx]\n",
        "        # Aplicamos a projeção final para voltar ao espaço do modelo\n",
        "        attn_output = self.c_proj(attn_output)  # [batch_size, n_ctx, nx]\n",
        "\n",
        "        return attn_output, present"
      ],
      "metadata": {
        "id": "YOq_jfcM7njk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, nx, n_ctx, n_head, scale=False):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        # Cria uma máscara de atenção triangular inferior (causal)\n",
        "        # Isso garante que cada token só preste atenção aos tokens anteriores\n",
        "        # Shape: [1, 1, n_ctx, n_ctx]\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx))\n",
        "\n",
        "        self.n_head = n_head  # Número de cabeças de atenção\n",
        "        self.split_size = nx  # Tamanho da dimensão do modelo\n",
        "        self.scale = scale    # Flag para aplicar escala nos scores de atenção\n",
        "\n",
        "        # Camada linear para projetar a entrada em Query, Key, Value para todas as cabeças\n",
        "        self.c_attn = Conv1D(nx * 3, nx)\n",
        "        # Camada linear para projetar a saída da atenção de volta ao espaço do modelo\n",
        "        self.c_proj = Conv1D(nx, nx)\n",
        "\n",
        "    def _attention(self, q, k, v):\n",
        "        # Calcula os scores de atenção: Query * Key^T\n",
        "        # Shape: [batch_size, n_head, seq_len, seq_len]\n",
        "        w = torch.matmul(q, k)\n",
        "\n",
        "        # Aplica escala opcional para estabilizar o treinamento\n",
        "        if self.scale:\n",
        "            w = w / math.sqrt(v.size(-1))\n",
        "\n",
        "        # Prepara e aplica a máscara causal\n",
        "        nd, ns = w.size(-2), w.size(-1)\n",
        "        mask = self.bias[:, :, ns-nd:ns, :ns]\n",
        "        w = w * mask - 1e10 * (1 - mask)  # Aplica -inf onde a máscara é 0\n",
        "\n",
        "        # Aplica softmax para obter os pesos de atenção\n",
        "        w = nn.Softmax(dim=-1)(w)\n",
        "\n",
        "        # Calcula a saída da atenção: pesos de atenção * Values\n",
        "        output = torch.matmul(w, v)\n",
        "        return output\n",
        "\n",
        "    def _merge_heads(self, x):\n",
        "        # Reorganiza o tensor de [batch, head, seq, features] para [batch, seq, head*features]\n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        new_x_shape = x.size()[:-2] + (x.size(-2) * x.size(-1),)\n",
        "        return x.view(*new_x_shape)\n",
        "\n",
        "    def _split_heads(self, x, k=False):\n",
        "        # Divide o último dimensão em [n_head, features/n_head]\n",
        "        new_x_shape = x.size()[:-1] + (self.n_head, x.size(-1) // self.n_head)\n",
        "        x = x.view(*new_x_shape)\n",
        "\n",
        "        # Reorganiza o tensor para [batch, head, seq, features/n_head]\n",
        "        if k:\n",
        "            # Para as keys, colocamos a dim seq por último para otimizar a multiplicação de matrizes\n",
        "            return x.permute(0, 2, 3, 1)\n",
        "        else:\n",
        "            return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, x, layer_past=None):\n",
        "        # x: entrada [batch_size, seq_len, nx]\n",
        "\n",
        "        # Projeta a entrada para Q, K, V de uma vez\n",
        "        qkv = self.c_attn(x)  # [batch_size, seq_len, nx*3]\n",
        "\n",
        "        # Separa Q, K, V\n",
        "        query, key, value = qkv.split(self.split_size, dim=2)\n",
        "        # query, key, value: cada um [batch_size, seq_len, nx]\n",
        "\n",
        "        # Divide as cabeças e reorganiza\n",
        "        query = self._split_heads(query)  # [batch_size, n_head, seq_len, nx/n_head]\n",
        "        key = self._split_heads(key, k=True)  # [batch_size, n_head, nx/n_head, seq_len]\n",
        "        value = self._split_heads(value)  # [batch_size, n_head, seq_len, nx/n_head]\n",
        "\n",
        "        # Lida com o cache do estado passado, se fornecido\n",
        "        if layer_past:\n",
        "            past_key, past_value = layer_past[0].transpose(-2, -1), layer_past[1]\n",
        "            key = torch.cat((past_key, key), dim=-1)  # [batch_size, n_head, nx/n_head, seq_len_extended]\n",
        "            value = torch.cat((past_value, value), dim=-2)  # [batch_size, n_head, seq_len_extended, nx/n_head]\n",
        "\n",
        "        # Armazena o estado atual para uso futuro\n",
        "        present = torch.stack((key.transpose(-2, -1), value))\n",
        "        # present: [2, batch_size, n_head, seq_len, nx/n_head]\n",
        "\n",
        "        # Calcula a atenção para todas as cabeças\n",
        "        attn_output = self._attention(query, key, value)\n",
        "        # [batch_size, n_head, seq_len, nx/n_head]\n",
        "\n",
        "        # Combina as cabeças novamente\n",
        "        attn_output = self._merge_heads(attn_output)  # [batch_size, seq_len, nx]\n",
        "\n",
        "        # Projeta de volta para o espaço do modelo\n",
        "        attn_output = self.c_proj(attn_output)  # [batch_size, seq_len, nx]\n",
        "\n",
        "        return attn_output, present"
      ],
      "metadata": {
        "id": "alztff99Kwmp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, n_state, n_embed):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        # n_state: geralmente 4 * n_embed, seguindo a arquitetura original do Transformer\n",
        "        # n_embed: dimensão do modelo (embedding dimension)\n",
        "\n",
        "        # Primeira camada linear: expande a dimensão\n",
        "        self.c_fc = Conv1D(n_state, n_embed)\n",
        "\n",
        "        # Segunda camada linear: projeta de volta para a dimensão original\n",
        "        self.c_proj = Conv1D(n_embed, n_state)\n",
        "\n",
        "        # Função de ativação GELU (Gaussian Error Linear Unit)\n",
        "        self.activation = gelu\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: entrada [batch_size, seq_len, n_embed]\n",
        "\n",
        "        # Aplica a primeira transformação linear e a função de ativação\n",
        "        h = self.c_fc(x)  # [batch_size, seq_len, n_state]\n",
        "        h = self.activation(h)  # [batch_size, seq_len, n_state]\n",
        "\n",
        "        # Aplica a segunda transformação linear\n",
        "        h2 = self.c_proj(h)  # [batch_size, seq_len, n_embed]\n",
        "\n",
        "        return h2  # [batch_size, seq_len, n_embed]"
      ],
      "metadata": {
        "id": "S56U3zCxN3HB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, n_ctx, config, scale=False):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "\n",
        "        nx = config.n_embed  # Dimensão do modelo\n",
        "\n",
        "        # Primeira camada de normalização, aplicada antes da atenção\n",
        "        self.ln_1 = LayerNorm(nx, eps=config.layer_norm_epsilon)\n",
        "\n",
        "        # Camada de atenção (neste caso, atenção de cabeça única)\n",
        "        self.attention = SingleHeadAttention(nx, n_ctx, config, scale)\n",
        "\n",
        "        # Segunda camada de normalização, aplicada antes da MLP\n",
        "        self.ln_2 = LayerNorm(nx, eps=config.layer_norm_epsilon)\n",
        "\n",
        "        # MLP (Feed-Forward Network)\n",
        "        self.mlp = MLP(4 * nx, nx)\n",
        "\n",
        "    def forward(self, x, layer_past=None):\n",
        "        # x: entrada [batch_size, seq_len, nx]\n",
        "\n",
        "        # Primeira normalização de camada\n",
        "        normalized_x = self.ln_1(x)  # [batch_size, seq_len, nx]\n",
        "\n",
        "        # Camada de atenção\n",
        "        attention_output, present = self.attention(normalized_x, layer_past=layer_past)\n",
        "        # attention_output: [batch_size, seq_len, nx]\n",
        "        # present: estado cacheado para geração incremental\n",
        "\n",
        "        # Conexão residual após a atenção\n",
        "        x = x + attention_output  # [batch_size, seq_len, nx]\n",
        "\n",
        "        # Segunda normalização de camada\n",
        "        normalized_x = self.ln_2(x)  # [batch_size, seq_len, nx]\n",
        "\n",
        "        # MLP\n",
        "        mlp_output = self.mlp(normalized_x)  # [batch_size, seq_len, nx]\n",
        "\n",
        "        # Conexão residual após a MLP\n",
        "        x = x + mlp_output  # [batch_size, seq_len, nx]\n",
        "\n",
        "        return x, present"
      ],
      "metadata": {
        "id": "XmPj9ydMOgL2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_layer = config.n_layer  # Número de camadas do Transformer\n",
        "        self.n_embed = config.n_embed  # Dimensão do embedding\n",
        "        self.n_vocab = config.n_vocab  # Tamanho do vocabulário\n",
        "        self.n_pos   = config.n_pos    # Número máximo de posições\n",
        "\n",
        "        # Embedding de tokens\n",
        "        self.wte = nn.Embedding(self.n_vocab, self.n_embed)\n",
        "        # Embedding de posições\n",
        "        self.wpe = nn.Embedding(self.n_pos, self.n_embed)\n",
        "\n",
        "        # Cria um bloco do Transformer\n",
        "        block = TransformerBlock(config.n_ctx, config, scale=True)\n",
        "\n",
        "        # Cria uma lista de blocos do Transformer\n",
        "        self.h = nn.ModuleList([copy.deepcopy(block) for _ in range(self.n_layer)])\n",
        "\n",
        "        # Camada final de normalização\n",
        "        self.ln_f = LayerNorm(self.n_embed, eps=config.layer_norm_epsilon)\n",
        "\n",
        "    def forward(self, input_ids, position_ids=None, token_type_ids=None, past=None):\n",
        "        # input_ids: [batch_size, seq_len]\n",
        "\n",
        "        if past is None:\n",
        "            past_length = 0\n",
        "            past = [None] * len(self.h)\n",
        "        else:\n",
        "            past_length = past[0][0].size(-2)\n",
        "\n",
        "        if position_ids is None:\n",
        "            # Gera ids de posição se não fornecidos\n",
        "            position_ids = torch.arange(past_length, input_ids.size(-1) + past_length, dtype=torch.long, device=input_ids.device)\n",
        "            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)  # [batch_size, seq_len]\n",
        "\n",
        "        input_shape = input_ids.size()\n",
        "        input_ids = input_ids.view(-1, input_ids.size(-1))  # [batch_size * seq_len]\n",
        "        position_ids = position_ids.view(-1, position_ids.size(-1))  # [batch_size * seq_len]\n",
        "\n",
        "        # Aplica embeddings de tokens e posições\n",
        "        input_embeds = self.wte(input_ids)  # [batch_size * seq_len, n_embed]\n",
        "        position_embeds = self.wpe(position_ids)  # [batch_size * seq_len, n_embed]\n",
        "\n",
        "        if token_type_ids is not None:\n",
        "            token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1))\n",
        "            token_type_embeds = self.wte(token_type_ids)  # [batch_size * seq_len, n_embed]\n",
        "        else:\n",
        "            token_type_embeds = 0\n",
        "\n",
        "        # Soma todos os embeddings\n",
        "        hidden_states = input_embeds + position_embeds + token_type_embeds  # [batch_size * seq_len, n_embed]\n",
        "\n",
        "        presents = []\n",
        "        for block, layer_past in zip(self.h, past):\n",
        "            hidden_states, present = block(hidden_states, layer_past)\n",
        "            # hidden_states: [batch_size * seq_len, n_embed]\n",
        "            presents.append(present)\n",
        "\n",
        "        # Aplica a normalização final\n",
        "        hidden_states = self.ln_f(hidden_states)  # [batch_size * seq_len, n_embed]\n",
        "\n",
        "        # Reshape para a forma original\n",
        "        output_shape = input_shape + (hidden_states.size(-1),)\n",
        "        hidden_states = hidden_states.view(*output_shape)  # [batch_size, seq_len, n_embed]\n",
        "\n",
        "        return hidden_states, presents"
      ],
      "metadata": {
        "id": "a8HrzC9JO-zG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MeCkqaE8TM1w"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Como aproveitar ao máximo sua assinatura do Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}